{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eefa1e15-9ef9-4a53-ba8c-d9a51eae4984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73947701-7091-4dba-a056-9bb7df469000",
   "metadata": {},
   "source": [
    "## 0. í•™ìŠµ ì¤€ë¹„\n",
    "- ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "- í™˜ê²½ ì„¤ì • (íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ë¡œë“œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d9f476-c678-40ec-8cae-acbe64b96a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í™˜ê²½ì„¤ì • ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# === ì…ì¶œë ¥ ê²½ë¡œ ===\n",
    "os.environ.setdefault(\"CSV_PATH\", \"schedule_dataset_augmented_epoch_2.csv\")   # seed íŒŒì¼ ê²½ë¡œ\n",
    "\n",
    "# ë°°ì¹˜ ë‹¨ìœ„ (í•œ ë²ˆì— ëª‡ ê°œì˜ í•­ëª©ì„ ìƒì„±ì‹œí‚¬ì§€)\n",
    "os.environ.setdefault(\"BATCH_SIZE\", \"10\")\n",
    "\n",
    "# ìƒì„± ë‹¤ì–‘ì„±\n",
    "os.environ.setdefault(\"TEMPERATURE\", \"0.8\")\n",
    "os.environ.setdefault(\"TOP_P\", \"0.9\")\n",
    "\n",
    "print(\"í™˜ê²½ì„¤ì • ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b24d0037-3b6f-4829-baa3-69876f9cfc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shape: (2000, 8)\n",
      "              sentence             domain    task  label  confidence   source  \\\n",
      "0     ë°°ê²½ ëŸ¬í”„ ìˆ˜ì •ë³¸ ì—¬ê¸° ìˆì–´ìš”  design_production    NONE      0        0.99  teacher   \n",
      "1  ì˜¤ëŠ˜ ë°¤ì— ìƒ‰ê° ì²´í¬ ê°€ëŠ¥í•˜ì‹ ê°€ìš”?  design_production  CREATE      1        0.92  teacher   \n",
      "2        ì‹±í¬í‘œ ë‹¤ì‹œ ì˜¬ë ¤ë“œë¦´ê²Œìš”  design_production    NONE      0        0.98  teacher   \n",
      "3   ê¸ˆìš”ì¼ ì˜¤í›„ í‚¥ì˜¤í”„ ë¯¸íŒ… í• ê¹Œìš”?  design_production  CREATE      1        0.95  teacher   \n",
      "4       ì˜¤ëŠ˜ ì½˜í‹° í”¼ë“œë°±ë§Œ ì£¼ì„¸ìš”  design_production    NONE      0        0.97  teacher   \n",
      "\n",
      "   seed_id  epoch  \n",
      "0      0.0      0  \n",
      "1      1.0      0  \n",
      "2      2.0      0  \n",
      "3      3.0      0  \n",
      "4      4.0      0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# í™˜ê²½ ì„¤ì •ê³¼ ë°ì´í„° ë¡œë“œ\n",
    "import re, random, json, types\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_PATH = os.getenv(\"CSV_PATH\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Loaded shape:\", df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab08fd4-d2f4-48fd-b66e-d24497f9f18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using columns -> text: sentence label: label\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1600, 400)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "# í…ìŠ¤íŠ¸/ë¼ë²¨ ì»¬ëŸ¼ ìë™ ê°ì§€ ë° ë¶„í• \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_col = \"sentence\" if \"sentence\" in df.columns else df.columns[0]\n",
    "label_col = \"label\" if \"label\" in df.columns else df.columns[-1]\n",
    "print(\"Using columns -> text:\", text_col, \"label:\", label_col)\n",
    "\n",
    "df = df[[text_col, label_col]].dropna().reset_index(drop=True)\n",
    "X = df[text_col].astype(str).values\n",
    "y = df[label_col].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=38, stratify=y\n",
    ")\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33139942-d8a1-4950-9287-da4645eb4dcb",
   "metadata": {},
   "source": [
    "## TF-IDF ë¶„ë¥˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81930558-7b31-41c4-aa0c-e17d654238a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFâ€‘IDF char Accuracy: 0.9925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       148\n",
      "           1       0.99      1.00      0.99       252\n",
      "\n",
      "    accuracy                           0.99       400\n",
      "   macro avg       0.99      0.99      0.99       400\n",
      "weighted avg       0.99      0.99      0.99       400\n",
      "\n",
      "TFâ€‘IDF word Accuracy: 0.9975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       148\n",
      "           1       1.00      1.00      1.00       252\n",
      "\n",
      "    accuracy                           1.00       400\n",
      "   macro avg       1.00      1.00      1.00       400\n",
      "weighted avg       1.00      1.00      1.00       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1) char ngram ê¸°ë°˜\n",
    "tfidf_char = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(analyzer=\"char\", ngram_range=(2,5), min_df=3)),\n",
    "    (\"clf\", LinearSVC())\n",
    "])\n",
    "tfidf_char.fit(X_train, y_train)\n",
    "pred_char = tfidf_char.predict(X_test)\n",
    "acc_char = accuracy_score(y_test, pred_char)\n",
    "print(\"TFâ€‘IDF char Accuracy:\", acc_char)\n",
    "print(classification_report(y_test, pred_char))\n",
    "\n",
    "# 2) word ê¸°ë°˜\n",
    "tfidf_word = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(analyzer=\"word\", token_pattern=r\"(?u)\\b\\w+\\b\", min_df=3)),\n",
    "    (\"clf\", LinearSVC())\n",
    "])\n",
    "tfidf_word.fit(X_train, y_train)\n",
    "pred_word = tfidf_word.predict(X_test)\n",
    "acc_word = accuracy_score(y_test, pred_word)\n",
    "print(\"TFâ€‘IDF word Accuracy:\", acc_word)\n",
    "print(classification_report(y_test, pred_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "538bb1c0-db7a-402b-a8df-993ad5b8b626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label_pred</th>\n",
       "      <th>agree_rate</th>\n",
       "      <th>certainty_avg</th>\n",
       "      <th>accept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/TS_1.shopping/01.ASë¬¸ì˜/shopping1_0625.txt</td>\n",
       "      <td>ë„¤ ë°˜ê°‘ìŠµë‹ˆë‹¤ ìƒë‹´ì‚¬ #@ì´ë¦„#ì…ë‹ˆë‹¤</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/TS_1.shopping/01.ASë¬¸ì˜/shopping1_0625.txt</td>\n",
       "      <td>ë„¤ ì•„ê¹Œ ì œê°€ ì§€ê¸ˆ ìƒë‹´ì„ í•˜ëŠ” ê±´ë° ë“¤ìœ¼ì„¸ìš”</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/TS_1.shopping/01.ASë¬¸ì˜/shopping1_0625.txt</td>\n",
       "      <td>#@ì†Œì†#ì—ì„œ íŒë§¤ë¥¼ ì–´ë–¤ ë¬¼ê±´ì„ í•˜ë“ ì§€ í•˜ê²Œ ë˜ë©´ì€ íŒë§¤ ì •ì±…ê³¼ ì‚¬ì—…ë¶€ê°€ ìˆì£ </td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/TS_1.shopping/01.ASë¬¸ì˜/shopping1_0625.txt</td>\n",
       "      <td>ìƒë‹´í•˜ëŠ” ê±°ëŠ” ê·¸ ì‚¬ì—…ì„ ìœ„í•´ì„œ ìƒë‹´í•˜ëŠ” ê±°ê³ ìš”</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/TS_1.shopping/01.ASë¬¸ì˜/shopping1_0625.txt</td>\n",
       "      <td>ê·¸ë‹ˆê¹Œ ì €í¬ëŠ” ìœ ì§€ê´€ë¦¬ë¥¼ ìƒë‹´í•´ ë“œë¦¬ëŠ” ìƒë‹´ì‹¤ì…ë‹ˆë‹¤</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file_path  \\\n",
       "0  test/TS_1.shopping/01.ASë¬¸ì˜/shopping1_0625.txt   \n",
       "1  test/TS_1.shopping/01.ASë¬¸ì˜/shopping1_0625.txt   \n",
       "2  test/TS_1.shopping/01.ASë¬¸ì˜/shopping1_0625.txt   \n",
       "3  test/TS_1.shopping/01.ASë¬¸ì˜/shopping1_0625.txt   \n",
       "4  test/TS_1.shopping/01.ASë¬¸ì˜/shopping1_0625.txt   \n",
       "\n",
       "                                       sentence  label_pred  agree_rate  \\\n",
       "0                          ë„¤ ë°˜ê°‘ìŠµë‹ˆë‹¤ ìƒë‹´ì‚¬ #@ì´ë¦„#ì…ë‹ˆë‹¤           0         1.0   \n",
       "1                     ë„¤ ì•„ê¹Œ ì œê°€ ì§€ê¸ˆ ìƒë‹´ì„ í•˜ëŠ” ê±´ë° ë“¤ìœ¼ì„¸ìš”           0         1.0   \n",
       "2  #@ì†Œì†#ì—ì„œ íŒë§¤ë¥¼ ì–´ë–¤ ë¬¼ê±´ì„ í•˜ë“ ì§€ í•˜ê²Œ ë˜ë©´ì€ íŒë§¤ ì •ì±…ê³¼ ì‚¬ì—…ë¶€ê°€ ìˆì£            0         1.0   \n",
       "3                    ìƒë‹´í•˜ëŠ” ê±°ëŠ” ê·¸ ì‚¬ì—…ì„ ìœ„í•´ì„œ ìƒë‹´í•˜ëŠ” ê±°ê³ ìš”           0         1.0   \n",
       "4                  ê·¸ë‹ˆê¹Œ ì €í¬ëŠ” ìœ ì§€ê´€ë¦¬ë¥¼ ìƒë‹´í•´ ë“œë¦¬ëŠ” ìƒë‹´ì‹¤ì…ë‹ˆë‹¤           0         1.0   \n",
       "\n",
       "   certainty_avg  accept  \n",
       "0             95    True  \n",
       "1             84    True  \n",
       "2             87    True  \n",
       "3             85    True  \n",
       "4             85    True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_labeled = pd.read_csv(\"labeled_sentences_voted.csv\")\n",
    "display(df_labeled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "765c5fb6-8790-475f-94ba-039b45724cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TF-IDF (char) ê²°ê³¼ ===\n",
      "Accuracy: 0.6827021494370522\n",
      "Confusion Matrix:\n",
      " [[611 291]\n",
      " [ 19  56]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.970     0.677     0.798       902\n",
      "           1      0.161     0.747     0.265        75\n",
      "\n",
      "    accuracy                          0.683       977\n",
      "   macro avg      0.566     0.712     0.532       977\n",
      "weighted avg      0.908     0.683     0.757       977\n",
      "\n",
      "\n",
      "=== TF-IDF (word) ê²°ê³¼ ===\n",
      "Accuracy: 0.34083930399181167\n",
      "Confusion Matrix:\n",
      " [[272 630]\n",
      " [ 14  61]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.951     0.302     0.458       902\n",
      "           1      0.088     0.813     0.159        75\n",
      "\n",
      "    accuracy                          0.341       977\n",
      "   macro avg      0.520     0.557     0.309       977\n",
      "weighted avg      0.885     0.341     0.435       977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# X, y ì„¤ì •\n",
    "X_test = df_labeled[\"sentence\"]\n",
    "y_test = df_labeled[\"label_pred\"]   # teacher ëª¨ë¸ë¡œ ë¼ë²¨ë§ëœ ê²°ê³¼\n",
    "\n",
    "# --- TF-IDF char ëª¨ë¸ ---\n",
    "pred_char = tfidf_char.predict(X_test)\n",
    "acc_char = accuracy_score(y_test, pred_char)\n",
    "print(\"\\n=== TF-IDF (char) ê²°ê³¼ ===\")\n",
    "print(\"Accuracy:\", acc_char)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, pred_char))\n",
    "print(classification_report(y_test, pred_char, digits=3))\n",
    "\n",
    "# --- TF-IDF word ëª¨ë¸ ---\n",
    "pred_word = tfidf_word.predict(X_test)\n",
    "acc_word = accuracy_score(y_test, pred_word)\n",
    "print(\"\\n=== TF-IDF (word) ê²°ê³¼ ===\")\n",
    "print(\"Accuracy:\", acc_word)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, pred_word))\n",
    "print(classification_report(y_test, pred_word, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d19b2eaf-37da-4dbd-ab0b-35513de73fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF-IDF char] thr=1.0867 (Pâ‰¥0.9) | VAL-like P=1.000, R=0.013\n",
      "TEST-ish  P=1.000 R=0.013 F1=0.026 PR-AUC=0.349\n",
      "[TF-IDF word] thr=1.5366 (Pâ‰¥0.9) | VAL-like P=1.000, R=0.013\n",
      "TEST-ish  P=1.000 R=0.013 F1=0.026 PR-AUC=0.299\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, precision_score, recall_score, f1_score, average_precision_score\n",
    "\n",
    "def pick_thr_for_precision(y_true, scores, target_p=0.90):\n",
    "    p, r, th = precision_recall_curve(y_true, scores)\n",
    "    cands = [(pp, rr, tt) for pp, rr, tt in zip(p[:-1], r[:-1], th) if pp >= target_p]\n",
    "    if cands:\n",
    "        # precision ë§Œì¡± ì¤‘ recall ìµœëŒ€\n",
    "        best = max(cands, key=lambda x: x[1])\n",
    "        return best[2], {\"P\": best[0], \"R\": best[1], \"picked\": f\"Pâ‰¥{target_p}\"}\n",
    "    # ì—†ìœ¼ë©´ F1 ìµœëŒ€ ì§€ì \n",
    "    f1s = []\n",
    "    for pp, rr, tt in zip(p[:-1], r[:-1], th):\n",
    "        f1 = 0 if (pp+rr)==0 else 2*pp*rr/(pp+rr)\n",
    "        f1s.append((f1, tt, pp, rr))\n",
    "    f1s.sort(reverse=True, key=lambda x: x[0])\n",
    "    f1, thr, pp, rr = f1s[0]\n",
    "    return thr, {\"P\": pp, \"R\": rr, \"F1\": f1, \"picked\": \"maxF1\"}\n",
    "\n",
    "def eval_with_thr(model, X, y, target_p=0.90, name=\"model\"):\n",
    "    scores = model.decision_function(X)  # LinearSVC ì ìˆ˜\n",
    "    thr, info = pick_thr_for_precision(y, scores, target_p)\n",
    "    y_pred = (scores >= thr).astype(int)\n",
    "    P = precision_score(y, y_pred, zero_division=0)\n",
    "    R = recall_score(y, y_pred, zero_division=0)\n",
    "    F1 = f1_score(y, y_pred, zero_division=0)\n",
    "    AP = average_precision_score(y, scores)\n",
    "    print(f\"[{name}] thr={thr:.4f} ({info['picked']}) | VAL-like P={info['P']:.3f}, R={info['R']:.3f}\")\n",
    "    print(f\"TEST-ish  P={P:.3f} R={R:.3f} F1={F1:.3f} PR-AUC={AP:.3f}\")\n",
    "    return thr, (P,R,F1,AP)\n",
    "\n",
    "# df_labeled ì „ì²´ê°€ í…ŒìŠ¤íŠ¸ì…‹ì´ë¼ë©´ ê·¸ëŒ€ë¡œ ì ìš© (í˜¹ì€ ë‚´ë¶€ì—ì„œ 80/20 ìª¼ê°œ ì„ê³„ê°’ ê³ ë¥´ê³  ë™ì¼ ë¶„í¬ì— í‰ê°€)\n",
    "thr_char, _ = eval_with_thr(tfidf_char, X_test, y_test, target_p=0.90, name=\"TF-IDF char\")\n",
    "thr_word, _ = eval_with_thr(tfidf_word, X_test, y_test, target_p=0.90, name=\"TF-IDF word\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34f250a0-a27b-44a9-9d32-230a231f219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "cal_char = CalibratedClassifierCV(tfidf_char, method=\"sigmoid\", cv=3).fit(X_train, y_train)\n",
    "probs = cal_char.predict_proba(X_test)[:,1]\n",
    "# â†’ precision ëª©í‘œì— ë§ì¶° threshold ì„ íƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4f19c64-ddf2-48e9-9f28-cc2cde02f116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Threshold] Pâ‰¥0.9 -> thr=0.9976  (VAL-like P=1.000, R=0.027)\n",
      "[TEST] Precision=1.000  Recall=0.027  F1=0.052  PR-AUC=0.389\n",
      "Confusion Matrix [rows=true 0/1, cols=pred 0/1]:\n",
      " [[902   0]\n",
      " [ 73   2]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.925     1.000     0.961       902\n",
      "           1      1.000     0.027     0.052        75\n",
      "\n",
      "    accuracy                          0.925       977\n",
      "   macro avg      0.963     0.513     0.507       977\n",
      "weighted avg      0.931     0.925     0.891       977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, precision_score, recall_score, f1_score,\n",
    "    average_precision_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "def pick_thr_for_precision(y_true, probs, target_p=0.90):\n",
    "    p, r, th = precision_recall_curve(y_true, probs)\n",
    "    # thresholds ê¸¸ì´ëŠ” p,rë³´ë‹¤ 1 ì‘ìŒ\n",
    "    cands = [(pp, rr, tt) for pp, rr, tt in zip(p[:-1], r[:-1], th) if pp >= target_p]\n",
    "    if cands:\n",
    "        # precision ë§Œì¡± ì¤‘ recall ìµœëŒ€ ì§€ì \n",
    "        best = max(cands, key=lambda x: x[1])\n",
    "        return best[2], {\"picked_by\": f\"Pâ‰¥{target_p}\", \"P\": best[0], \"R\": best[1]}\n",
    "    # fallback: F1 ìµœëŒ€ ì§€ì \n",
    "    f1s = []\n",
    "    for pp, rr, tt in zip(p[:-1], r[:-1], th):\n",
    "        f1 = 0 if (pp+rr)==0 else 2*pp*rr/(pp+rr)\n",
    "        f1s.append((f1, tt, pp, rr))\n",
    "    f1s.sort(reverse=True, key=lambda x: x[0])\n",
    "    f1, thr, pp, rr = f1s[0]\n",
    "    return thr, {\"picked_by\": \"maxF1\", \"P\": pp, \"R\": rr, \"F1\": f1}\n",
    "\n",
    "# 1) ì„ê³„ê°’ ì„ íƒ (ì˜ˆ: Precision 0.90 ëª©í‘œ)\n",
    "thr, info = pick_thr_for_precision(y_test, probs, target_p=0.9)\n",
    "print(f\"[Threshold] {info['picked_by']} -> thr={thr:.4f}  (VAL-like P={info['P']:.3f}, R={info['R']:.3f})\")\n",
    "\n",
    "# 2) í…ŒìŠ¤íŠ¸ì…‹ ì˜ˆì¸¡ ë° ì§€í‘œ\n",
    "y_pred = (probs >= thr).astype(int)\n",
    "\n",
    "P = precision_score(y_test, y_pred, zero_division=0)\n",
    "R = recall_score(y_test, y_pred, zero_division=0)\n",
    "F1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "AP = average_precision_score(y_test, probs)  # PR-AUCì€ í™•ë¥  ê¸°ë°˜\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "\n",
    "print(f\"[TEST] Precision={P:.3f}  Recall={R:.3f}  F1={F1:.3f}  PR-AUC={AP:.3f}\")\n",
    "print(\"Confusion Matrix [rows=true 0/1, cols=pred 0/1]:\\n\", cm)\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79dcf983-959f-4693-b9b2-3a3068f20e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using columns -> text: sentence label: label\n",
      "['ë°°ê²½ ëŸ¬í”„ ìˆ˜ì •ë³¸ ì—¬ê¸° ìˆì–´ìš”' 'ì˜¤ëŠ˜ ë°¤ì— ìƒ‰ê° ì²´í¬ ê°€ëŠ¥í•˜ì‹ ê°€ìš”?' 'ì‹±í¬í‘œ ë‹¤ì‹œ ì˜¬ë ¤ë“œë¦´ê²Œìš”' ...\n",
      " 'ì½˜í‹° ì´ˆì•ˆ ì¢€ ë” ë¹¨ë¦¬ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”? ğŸ™ƒ' 'ê¸ˆìš”ì¼ ë¦¬ë·° ì‹œê°„, 15ë¶„ ëŠ¦ì¶°ë„ ê´œì°®ì•„? ë‚´ì¼ ì•„ì¹¨ì— ë‹¤ì‹œ ì¡°ìœ¨í•´ë³¼ê²Œ.'\n",
      " 'ì˜¤ëŠ˜ ì½˜í‹° í”¼ë“œë°± ì¢€ ì£¼ì‹¤ë˜ìš”? ğŸ‘€'] [0 1 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    " # í…ìŠ¤íŠ¸/ë¼ë²¨ ì»¬ëŸ¼ ìë™ ê°ì§€ ë° ë¶„í• \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_col = \"sentence\" if \"sentence\" in df.columns else df.columns[0]\n",
    "label_col = \"label\" if \"label\" in df.columns else df.columns[-1]\n",
    "print(\"Using columns -> text:\", text_col, \"label:\", label_col)\n",
    "\n",
    "df = df[[text_col, label_col]].dropna().reset_index(drop=True)\n",
    "X_test = df[text_col].astype(str).values\n",
    "Y_test = df[label_col].values\n",
    "\n",
    "print(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de95d111-6955-43e9-b101-0ddad0e38487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFâ€‘IDF char Accuracy: 0.9985\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       740\n",
      "           1       1.00      1.00      1.00      1260\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3) ê¸°ì¡´ ë°ì´í„°ì…‹ í‰ê°€\n",
    "\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "model = joblib.load(\"artifacts/char_calibrated.joblib\")\n",
    "\n",
    "pred_char = model.predict(X_test)\n",
    "acc_char = accuracy_score(Y_test, pred_char)\n",
    "print(\"TFâ€‘IDF char Accuracy:\", acc_char)\n",
    "print(classification_report(Y_test, pred_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8018e-1c27-4ab7-8e1a-0a4f0ca29c82",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ ë‚´ë³´ë‚´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "593fa65c-2db6-47af-8fb4-131a28dd6ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: /home/j-k13s101/artifacts/char_calibrated.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib, os\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "joblib.dump(cal_char, \"artifacts/char_calibrated.joblib\")\n",
    "print(\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ:\", os.path.abspath(\"artifacts/char_calibrated.joblib\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
